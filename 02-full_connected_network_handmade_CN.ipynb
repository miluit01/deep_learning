{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 手工初级练丹\n",
    "\n",
    "使用经典nmist数据集，训练数字识别的模型\n",
    "\n",
    "* 没有使用keras的封装\n",
    "* 用tf给的函数和数据类型构建一个3层的全链接网络"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "source": [
    "## 准备数据集\n",
    "\n",
    "这个地方还是用了一个Keras封装，自动处理：下载nmist的数据集，读入内存，转成tensor数据格式，分成训练和测试两部分。其实自己手写也不是很难。有时间手工实现一下。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='./nmist.npz')\n",
    "\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "\n",
    "#print(x_train.shape, y_train.shape, y_train.dtype, x_test.shape, y_test.shape, y_test.dtype)"
   ]
  },
  {
   "source": [
    "为了后面的测试数据对比的简便，这里就不对测试集中的输出真值做one_hot了"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test = tf.one_hot(y_test, depth=10)"
   ]
  },
  {
   "source": [
    "## 创建数据包Dataset\n",
    "\n",
    "对齐好的训练/测试数据TensorSliceDataset。原因很简单: <br/>\n",
    "\n",
    "x_train [60k, 28, 28] y_train [60k,] 捆绑后，利于后期对数据的整体同步修改。 \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "source": [
    "传递一个callback函数进入Dataset的map函数里，对数据的值域和类型做改变 <br/>\n",
    "这里的def preprocess就是把训练集中x的原有int32转换到0.0-1.0之间的float32， y值改成了int64"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    #x = 1. + tf.cast(x, tf.float32) / 255.0 - 2 # [-1., 1.]\n",
    "    x = tf.cast(x, tf.float32) / 255.0 # [0. , 1.]\n",
    "    y = tf.cast(y, dtype=tf.int64)\n",
    "    return x, y\n"
   ]
  },
  {
   "source": [
    "* 打乱 -> shuffle\n",
    "* 做批处理包 -> batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = db_train.map(preprocess).shuffle(60000).batch(100) \n",
    "db_test = db_test.map(preprocess).shuffle(10000).batch(100)\n"
   ]
  },
  {
   "source": [
    "网络的向前传播 y = w*x + b，所需要的参数w: 节点链接的权重, b: bias误差参数，这里定义了一个三层的全连接网; 所有可以更新的参数，如w，b必须用tf.Variable封装下，以便被全局梯度函数监视"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random.truncated_normal([28*28, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n"
   ]
  },
  {
   "source": [
    "学习率，非常有用，是解决梯度爆炸和迷散的关键因素。一般来说用一个小点的开始练丹。不行了上，下调一下。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n"
   ]
  },
  {
   "source": [
    "## 练丹开始"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "epoch: 纪元，一组数据该练多少次\n",
    "step: 第二个for里的这个step是根据上边定义batch的时候自动算出的，比如60K张图batch(10K),那一个纪元就得分6个steps搞完\n",
    "\n",
    "*先把输入的图像张量里的数据轴(2,3两轴)敲成1维的\n",
    "*正向传播了，就是乘法，加法然后激活函数\n",
    "*算出的结果和真值做损失计算，这里使用了分类交叉熵函数，因为，mnist就是一个解决分类问题的模型\n",
    "*用损失函数的结果计算坡度\n",
    "*反向传播，使用梯度更新所有的网络参数，注意这里一定要使用assign_sub不然，参数的对象会不停的被重新创建，这样tape就不能监视值的变动了\n",
    "\n",
    "这里尝试了MSE(mean squared error)均值平方差，和CCE(categorical crossentropy)分类交叉熵作为损失函数. 在训练的时候表现了明显的差别。<br/>\n",
    "CCE收敛速度明显更快，准确度提升的也很快，基本3-5纪元就可以到达95%的准确度。总体50个纪元差不多能到98%. 而MSE 50个纪元只能到达80%左右。\n",
    "\n",
    "每个纪元完毕后，检查一次作业，根据准确数和测试样本数确认准确率"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 1 batch: 1 loss:  2.4292261600494385\n",
      "epoch: 1 batch: 101 loss:  0.540456235408783\n",
      "epoch: 1 batch: 201 loss:  0.2897457778453827\n",
      "epoch: 1 batch: 301 loss:  0.2927491366863251\n",
      "epoch: 1 batch: 401 loss:  0.24325443804264069\n",
      "epoch: 1 batch: 501 loss:  0.3704439103603363\n",
      "test acc: 94.1 %\n",
      "epoch: 2 batch: 1 loss:  0.1627996414899826\n",
      "epoch: 2 batch: 101 loss:  0.15087324380874634\n",
      "epoch: 2 batch: 201 loss:  0.12699632346630096\n",
      "epoch: 2 batch: 301 loss:  0.13553215563297272\n",
      "epoch: 2 batch: 401 loss:  0.20415328443050385\n",
      "epoch: 2 batch: 501 loss:  0.20830899477005005\n",
      "test acc: 95.48 %\n",
      "epoch: 3 batch: 1 loss:  0.1231527253985405\n",
      "epoch: 3 batch: 101 loss:  0.10673525929450989\n",
      "epoch: 3 batch: 201 loss:  0.07322080433368683\n",
      "epoch: 3 batch: 301 loss:  0.09598022699356079\n",
      "epoch: 3 batch: 401 loss:  0.16377310454845428\n",
      "epoch: 3 batch: 501 loss:  0.09559107571840286\n",
      "test acc: 96.09 %\n",
      "epoch: 4 batch: 1 loss:  0.10920644551515579\n",
      "epoch: 4 batch: 101 loss:  0.08848544955253601\n",
      "epoch: 4 batch: 201 loss:  0.10470050573348999\n",
      "epoch: 4 batch: 301 loss:  0.0634792372584343\n",
      "epoch: 4 batch: 401 loss:  0.11933683604001999\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cbd1b34c9b79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Training...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2605\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2607\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2608\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    # Training...\n",
    "    for step, (x, y) in enumerate(db_train):\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            h1 = tf.nn.relu(x@w1 + b1)\n",
    "            h2 = tf.nn.relu(h1@w2 + b2)\n",
    "            out = h2@w3 + b3\n",
    "            loss = tf.losses.categorical_crossentropy(y, out, from_logits=True)\n",
    "            #loss = tf.reduce_mean(tf.square(tf.cast(y, dtype=tf.float32) - out))\n",
    "        \n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        \n",
    "        w1.assign_sub(lr * grads[0])\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "        \n",
    "        w2.assign_sub(lr * grads[2])\n",
    "        b2.assign_sub(lr * grads[3])\n",
    "        \n",
    "        w3.assign_sub(lr * grads[4])\n",
    "        b3.assign_sub(lr * grads[5])\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('epoch:', epoch+1, 'batch:', step+1, 'loss: ', float(tf.reduce_mean(loss)))\n",
    "\n",
    "    # Testing...\n",
    "    total_correct, total_num = 0, 0\n",
    "    for step, (x, y) in enumerate(db_test):\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        \n",
    "        h1 = tf.nn.relu(x@w1 + b1)\n",
    "        h2 = tf.nn.relu(h1@w2 + b2)\n",
    "        out = h2@w3 + b3\n",
    "\n",
    "        prob = tf.nn.softmax(out, axis=1)\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        \n",
    "        total_correct += int(correct)\n",
    "        total_num += x.shape[0]\n",
    "\n",
    "    print('test acc:', total_correct * 100 / total_num, '%')"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}