{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 手工打造一个网络骨架backbone\n",
    "\n",
    "一个复杂深度神经网络的backbone，也就是骨架，他决定了数据中的features是如何在网络里被层层剥离出来的。关于图形识别这个领域里不同的任务和要求有不同的算法实现，也就是有不同的backbone可供选择。\n",
    "\n",
    "在输入给定的情况下，如何处理这些输入和如何去正/反向在网络中传播数据，更新权重和偏差值，会给网络的效率和最终的准确度带来很大的影响。\n",
    "\n",
    "作为深度学习的进阶挑战就是用tensorflow的api把别人的算法作为一个网络模型实现出来。没什么神奇的。难的是怎么设计一个算法。我不是算法大神，只是希望能读懂别人的论文，然后用代码实现出来。所以手动实现ResNet就是一个好的练手。\n",
    "\n",
    "ResNet [https://arxiv.org/pdf/1512.03385.pdf] 残差神经网络是一种算法，是一个高阶，或者是高优化的卷积神经网络。在卷积层间加入bypass->优化模型训练时收敛速度，提高精度。使更深的网络有了意义 （理论上大于22层的经典卷积网络在性能上已经没有没有的明显的提高了，反而加大了训练的负担，使网络在应用时也需要更多的资源）。\n",
    "\n",
    "卷积网络的尾部可以安装一套全连接的层，卷积层中发现的features作为全连接层的输入，输出最终的分类结果。这套全连接层通常被称为网络的header。其实也有很复杂的header，其中包含卷积和上下采样，等等复杂的预处理，再做最后的分类任务。这里就暂时这么理解吧。\n",
    "\n",
    "## 第一步 - 实现基本的残差block\n",
    "\n",
    "ResNet有基本的参差块组成 -> 2个及以上的卷积层组成，层之间是Relu。block的入口和出口之间直接有一个identity做成的bypass\n",
    "\n",
    "![Residual block](pic/resnet_single_block.png)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers, regularizers, Sequential\n",
    "\n",
    "initializer = tf.random_normal_initializer(stddev=0.01)\n",
    "regularizer = tf.keras.regularizers.l2(4e-5)\n",
    "\n",
    "class ResBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self, filters, strides=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        # the first conv layer could downsample the input via strides\n",
    "        self.conv1 = layers.Conv2D(filters, \n",
    "                                   (3, 3), \n",
    "                                   strides=strides, \n",
    "                                   padding='same'，\n",
    "                                   kernel_initializer=initializer, \n",
    "                                   kernel_regularizer=regularizer)\n",
    "        self.bn1   = layers.BatchNormalization()\n",
    "        self.relu  = layers.Activation('relu')\n",
    "        \n",
    "        # the second conv layer need to keep the output of first conv layer\n",
    "        # therefore the strides must be 1\n",
    "        self.conv2 = layers.Conv2D(filters, \n",
    "                                   (3, 3), \n",
    "                                   strides=1, \n",
    "                                   padding='same', \n",
    "                                   kernel_initializer=initializer, \n",
    "                                   kernel_regularizer=regularizer)\n",
    "        self.bn2   = layers.BatchNormalization()\n",
    "        \n",
    "        # in case the strides equal 1 the identity function return the input of the block\n",
    "        if strides == 1:\n",
    "            self.identity = lambda x:x\n",
    "        # if strides greater than 1 means we have to convert the input with the same shape\n",
    "        # as the processed input\n",
    "        # e.g.: strides = [2, 2] input [b, 28, 28]\n",
    "        # output conv1: [b, 27, 27] -> conv2: [b, 27, 27]\n",
    "        else:\n",
    "            self.identity = Sequential([layers.Conv2D(filters, (1, 1), strides)])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        conv1_out = self.relu(self.bn1(self.conv1(inputs), training=training))\n",
    "        conv2_out = self.bn2(self.conv2(conv1_out), training=training)\n",
    "\n",
    "        identity = self.identity(inputs)\n",
    "        output = tf.nn.relu(layers.add([conv2_out, identity]))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "source": [
    "## 整体残差网络模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(models.Model):\n",
    "\n",
    "    def __init__(self, dims, class_numbers):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.stem = Sequential([\n",
    "                                layers.Conv2D(64, \n",
    "                                              (7, 7), \n",
    "                                              strides=2, \n",
    "                                              padding='same', \n",
    "                                              kernel_initializer=initializer),\n",
    "                                layers.BatchNormalization(),\n",
    "                                layers.Activation('relu'),\n",
    "                                layers.MaxPool2D(pool_size=(2,2), strides=1, padding='same')\n",
    "                                ])\n",
    "\n",
    "        self.layer1 = self.build_resblocks(64, dims[0])\n",
    "        self.layer2 = self.build_resblocks(128, dims[1], strides=2)\n",
    "        self.layer3 = self.build_resblocks(256, dims[2], strides=2)\n",
    "        self.layer4 = self.build_resblocks(512, dims[3], strides=2)\n",
    "\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        self.fc = layers.Dense(class_numbers)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.stem(inputs)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_resblocks(self, filters, numbers, strides=1):\n",
    "        res_blocks = Sequential([ResBlock(filters, strides)])\n",
    "\n",
    "        for _ in range(1, numbers):\n",
    "            res_blocks.add(ResBlock(filters, strides=1))\n",
    "\n",
    "        return res_blocks"
   ]
  },
  {
   "source": [
    "## 18和34层的ResNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(class_numbers):\n",
    "    return ResNet(dims=[2, 2, 2, 2], class_numbers=class_numbers)\n",
    "\n",
    "\n",
    "def resnet34(class_numbers):\n",
    "    return ResNet(dims=[3, 4, 6, 3], class_numbers=class_numbers)\n"
   ]
  }
 ]
}